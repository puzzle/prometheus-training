---
title: "8.1 Tasks: Application Monitoring"
weight: 8
sectionnumber: 8.1
---

### Task {{% param sectionnumber %}}.1: Create a ServiceMonitor

**Task description**:

Create a ServiceMonitor  for the example application

* Create a ServiceMonitor, which will configure Prometheus to scrape metrics from the example-web-python application every 30 seconds.
  * hint: `kubectl -n application-metrics apply -f my_file.yaml` will create a resource in the Kubernetes namespace

For this to work, you need to ensure:

* The example-web-python Service is labeled correctly and matches the labels you've defined in your ServiceMonitor.
* The port name in your ServiceMonitor configuration matches the port name in the Service definition.
  * hint: check with `kubectl get service example-web-python -n application-metrics -o yaml`
* Verify the target in the Prometheus user interface

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

Create the following ServiceMonitor (`~/work/servicemonitor.yaml`) in the `application-metrics` namespace

{{< highlight yaml >}}{{< readfile file="content/en/docs/08/labs/servicemonitor.yaml" >}}{{< /highlight >}}

Apply it using the following command:

```bash
kubectl -n application-metrics apply -f ~/work/servicemonitor.yaml
```

Verify that the target gets scraped in the [Prometheus user interface](http://LOCALHOST:19090/targets). Target name: `application-metrics/example-web-python-monitor/0` (It may take up to a minute for Prometheus to load the new
configuration and scrape the metrics).

{{% /details %}}

### Task {{% param sectionnumber %}}.2: Deploy a database and use a sidecar container to expose metrics

**Task description**:

As we've learned in [Lab 4 - Prometheus exporters](../../../04/) when applications do not expose metrics in the Prometheus format, there are a lot of exporters available to convert metrics into the correct format. In Kubernetes this is often done by deploying so called sidecar containers along with the actual application.

Use the following command to deploy a MariaDB database in the `application-metrics` namespace.

```bash
curl -o ~/work/mariadb.yaml \
https://raw.githubusercontent.com/puzzle/prometheus-training/main/content/en/docs/08/labs/mariadb.yaml
kubectl -n application-metrics apply -f ~/work/mariadb.yaml
```

This will create a [Secret](https://kubernetes.io/docs/concepts/configuration/secret/) (username password to access the database), a [Service](https://kubernetes.io/docs/concepts/services-networking/service/) and the [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/).

* Deploy the [mariadb exporter](https://github.com/prometheus/mysqld_exporter) from <https://registry.hub.docker.com/r/prom/mysqld-exporter/> as a sidecar container
  * Alter the existing MariaDB deployment definition (~/work/mariadb.yaml) to contain the side car
  * Apply your changes to the cluster with `kubectl -n application-metrics apply -f ~/work/mariadb.yaml`
* Create a ServiceMonitor to instruct Prometheus to scrape the sidecar container

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

First we need to alter the deployment of the MariaDB with adding the MariaDB exporter as a second container.
Then extend the service by adding a second port for the MariaDB exporter.

{{< highlight yaml >}}{{< readfile file="content/en/docs/08/labs/mariadb-sidecar.yaml" >}}{{< /highlight >}}

We can apply the file above using:

```bash
kubectl -n application-metrics apply -f ~/work/mariadb.yaml
```

Then we also need to create a new ServiceMonitor `~/work/servicemonitor-sidecar.yaml`.

{{< highlight yaml >}}{{< readfile file="content/en/docs/08/labs/servicemonitor-sidecar.yaml" >}}{{< /highlight >}}

```bash
kubectl -n application-metrics apply -f ~/work/servicemonitor-sidecar.yaml
```

Verify that the target gets scraped in the [Prometheus user interface](http://LOCALHOST:19090/targets). Target name: `application-metrics/mariadb/0` (It may take up to a minute for Prometheus to load the new configuration and
scrape the metrics).

{{% /details %}}

### Task {{% param sectionnumber %}}.4: Troubleshooting Kubernetes Service Discovery

we will now deploy an application with an error in the monitoring configration.

* Deploy [loki](https://grafana.com/oss/loki/) in the application-metrics namespace

```bash
kubectl -n application-metrics create deployment loki \
--image=mirror.gcr.io/grafana/loki:latest
```

* Create a Service for loki

{{< highlight yaml >}}{{< readfile file="content/en/docs/08/labs/service-loki.yaml" >}}{{< /highlight >}}

```bash
kubectl -n application-metrics create -f svc.yaml
```

* Create the following ServiceMonitor

{{< highlight yaml >}}{{< readfile file="content/en/docs/08/labs/servicemonitor-loki.yaml" >}}{{< /highlight >}}

```bash
kubectl -n application-metrics create -f servicemonitor.yaml
```

* Try to find out why Prometheus cannot scrape metrics from the target

{{% alert title="Troubleshooting: Prometheus is not scrapping metrics" color="primary" %}}

The configuration defined through the ServiceMonitor does not appear in the Prometheus scrape config

* Check if the label of your ServiceMonitor matches the label defined in the Prometheus custom resource
* Check the Prometheus operator logs for errors (Permission issues or invalid ServiceMonitors)

The Endpoint appears in the Prometheus scrape config but not under targets. The Service Discovery can't find the Endpoint.

* The namespaceSelector in the ServiceMonitor does not match the namespace of your app
* The label selector does not match the Service of your app
* The port name does not match the Service of your app

The Endpoint appears in Prometheus, but no data gets scraped

* The application does not provide metrics under the correct path and port
* Networking issues
* Authentication required, but not configured

{{% /alert %}}

{{% details title="Hints" mode-switcher="normalexpertmode" %}}


Let's first find out which of the following statements apply to us

* The configuration defined through the ServiceMonitor does not appear in the Prometheus scrape config
* The Endpoint appears in the Prometheus scrape config but not under targets. The Service Discovery can't find the Endpoint.
* The Endpoint appears in Prometheus under Targets, but no data gets scraped


#### Check port / path

Let's check if the application provides metrics on port `3100` and under path `/metrics` as expected

```bash
kubectl -n application-metrics exec loki-5846d87f4c-4794j -it -- sh
/tmp $ wget localhost:3100/metrics
Connecting to localhost:3100 (127.0.0.1:3100)
saving to 'metrics'
metrics              100% |**********************************************************************************************************************************************************************************************| 82043  0:00:00 ETA
'metrics' saved
/tmp $ cat metrics
# HELP cortex_cache_corrupt_chunks_total Total count of corrupt chunks found in cache.
# TYPE cortex_cache_corrupt_chunks_total counter
cortex_cache_corrupt_chunks_total 0
# HELP cortex_chunk_store_chunks_per_query Distribution of #chunks per query.
# TYPE cortex_chunk_store_chunks_per_query histogram
cortex_chunk_store_chunks_per_query_bucket{le="10"} 0
cortex_chunk_store_chunks_per_query_bucket{le="80"} 0
cortex_chunk_store_chunks_per_query_bucket{le="640"} 0
```

#### Check servicemonitor <> Prometheus

Let's check if  Prometheus reads the configuration defined in the ServiceMonitor resource. To do so navigate to [Prometheus config](http://LOCALHOST:19090/config) and search if `loki` appears in the scrape_configuration.

#### Check servicemonitor <> svc

The application exposes metrics and the Prometheus generated the configuration according to the defined servicemonitor. Let's check, if the scrape configuration does

[Prometheus Service Discovery](http://LOCALHOST:19090/service-discovery)

{{% /details %}}

### Task {{% param sectionnumber %}}.4: Blackbox monitoring in Kubernetes (optional)

In [Lab 4 - Prometheus exporters](../../../04/) we came across the blackbox exporter and learned how configuring a [multi-target exporter through relabel_configs](https://prometheus.io/docs/guides/multi-target-exporter/) can be a bit tricky to understand. The Prometheus operator brings us a so-called Probe custom resource, which allows us to define the targets for a black box exporter in a much simplified way.

**Task description**:

* Create a [Probe custom resource](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/design.md#probe) in the application-metrics namespace for the example-web-python application
* Use the Prometheus expression browser to check if the new metric is being scraped

{{% alert title="Note" color="primary" %}}

Use `kubectl describe crd probe | less` to describe the crd and get the available options.

{{% /alert %}}

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

Create the following probe custom resource (`~/work/probe.yaml`) in the `application-metrics` namespace

{{< highlight yaml >}}{{< readfile file="content/en/docs/08/labs/probe.yaml" >}}{{< /highlight >}}

Apply it using the following command:

```bash
kubectl -n application-metrics apply -f ~/work/probe.yaml
```

Verify that the target gets scraped in the [Prometheus user interface](http://LOCALHOST:19090/targets). Target name: `application-metrics/example-web-python-probe` (It may take up to a minute for Prometheus to load the new
configuration and scrape the metrics).

Check for the following metric in Prometheus:

```promql
{instance="example-web-python.application-metrics.svc:5000/health"}
```
{{% /details %}}
