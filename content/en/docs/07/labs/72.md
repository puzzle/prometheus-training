---
title: "7.2 Tasks: kube-prometheus setup"
weight: 7
sectionnumber: 7.2
---

{{% onlyWhenNot baloise %}}

The [kube-prometheus](https://github.com/prometheus-operator/kube-prometheus) stack already provides an extensive Prometheus setup and contains a set of default alerts and dashboards from [Prometheus Monitoring Mixin for Kubernetes](https://github.com/kubernetes-monitoring/kubernetes-mixin). The following targets will be available.

{{% /onlyWhenNot %}}

{{% onlyWhen baloise %}}

The [Baloise Monitoring Stack](https://confluence.baloisenet.com/atlassian/display/BALMATE/06+-+Monitor+your+application+using+the+Baloise+Monitoring+Stack) stack already provides an extensive Prometheus setup and contains a set of default alerts and dashboards. The following targets will be available.

{{% /onlyWhen %}}


**kube-state-metrics:** Exposes metadata information about Kubernetes resources. Used, for example, to check if resources have the expected state (deployment rollouts, pods CrashLooping) or if jobs fail.

```promql
# Example metrics
kube_deployment_created
kube_deployment_spec_replicas
kube_daemonset_status_number_misscheduled
...
```

**kubelet/cAdvisor:** [Advisor](https://github.com/google/cadvisor) exposes usage and performance metrics about running container. Commonly used to observe memory usage or [CPU throttling](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/).

```promql
# Example metrics
container_cpu_cfs_throttled_periods_total
container_memory_working_set_bytes
container_fs_inodes_free
...
```

**kubelet:** Exposes general kubelet related metrics. Used to observe if the kubelet and the container engine is healthy.

```promql
# Example metrics
kubelet_runtime_operations_duration_seconds_bucket
kubelet_runtime_operations_total
...
```

{{% onlyWhenNot baloise %}}
**apiserver:** Metrics from the Kubernetes API server. Commonly used to catch errors on resources or problems with latency.

```promql
# Example metrics
apiserver_request_duration_seconds_bucket
apiserver_request_total{code="200",...}
...
```
{{% /onlyWhenNot %}}

**kubelet/probes:** Expose metrics about [Kubernetes liveness, readiness and startup probes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/) Normally you would not alert on Kubernetes probe metrics, but on container restarts exposed by `kube-state-metrics`.

```promql
# Example metrics
prober_probe_total{probe_type="Liveness", result="successful",...}
prober_probe_total{probe_type="Startup", result="successful",...}
...
```

**blackbox-exporter:** Exposes default metrics from blackbox-exporter. Can be customized using the [Probe](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#probe) custom resource.

```promql
# Example metrics
probe_http_status_code
probe_http_duration_seconds
...
```

{{% onlyWhenNot baloise %}}
**node-exporter:** Exposes the hardware and OS metrics from the nodes running Kubernetes.

```promql
# Example metrics
node_filesystem_avail_bytes
node_disk_io_now
...
```
{{% /onlyWhenNot %}}

**alertmanager-main/grafana/prometheus-k8s/prometheus-operator/prometheus-adapter:** Exposes all monitoring stack component metrics.

```promql
# Example metrics
alertmanager_alerts_received_total
alertmanager_config_last_reload_successful
...
grafana_build_info
grafana_datasource_request_total
...
prometheus_config_last_reload_successful
prometheus_rule_evaluation_duration_seconds
...
prometheus_operator_reconcile_operations_total
prometheus_operator_managed_resources
...
```

{{% onlyWhen baloise %}}
**pushgateway:** Exposes metrics pushed to your pushgateway.

```promql
# Example metrics
pushgateway_build_info
pushgateway_http_requests_total
...
```

**Vault:** Exposes metrics from your vaults instance.

```promql
# Example metrics
vault_autopilot_healthy
vault_core_unsealed
...
```

**Jenkins:** Exposes metrics from your Jenkins instance.

```promql
# Example metrics
jenkins_builds_duration_milliseconds_summary_count
jenkins_executors_available
...
```

**Argo CD:** Exposes metrics from your Argo CD instance.


```promql
# Example metrics
argocd_app_info{sync_status="Synced", health_status="Healthy", ...}
argocd_app_sync_total
...
```

### Task {{% param sectionnumber %}}.1: Container restart alerting rule

Navigate to the [Thanos Ruler user interface](http://{{% param replacePlaceholder.thanos %}}/rules) and take a look at the provided default Prometheus rules.

**Task description**:

* Investigate if there is a default Alerting rule configured to monitor container restarts
* Which exporter exposes the required metrics?

{{% alert title="Note" color="primary" %}}
Search for an alert with `restart` in its name
{{% /alert %}}

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

The Alerting rule is called `PodRestartCount` and the PromQL defined for the rule looks as follows:

```promql
:kube_pod_container_status_restarts_total:offset
  * on(namespace, cluster) group_left(env) kube_namespace_labels{label_monitoringselector="<team>-monitoring"}
    and on(pod, namespace) kube_pod_labels{label_app_kubernetes_io_part_of!="bdop"}
```
If you take a look at the labels, you will see that the metric is provided by the `kube-state-metrics` exporter. (`..,job="kube-state-metrics",..`)

{{% /details %}}

{{% /onlyWhen %}}

{{% onlyWhenNot baloise %}}

### Task {{% param sectionnumber %}}.1: Container restart alerting rule

Navigate to the [Prometheus user interface](http://{{% param replacePlaceholder.k8sPrometheus %}}/rules) and take a look at the provided default Prometheus rules.

**Task description**:

* Investigate if there is a default Alerting rule configured to monitor container restarts
* Which exporter exposes the required metrics?

{{% alert title="Note" color="primary" %}}
Search for an alert with `CrashLooping` in its name
{{% /alert %}}

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

The Alerting rule is called `KubePodCrashLooping` and the PromQL defined for the rule looks as follows:

```promql
max_over_time(kube_pod_container_status_waiting_reason{job="kube-state-metrics",reason="CrashLoopBackOff"}[5m]) >= 1
```
If you take a look at the query, you will see that there is a filter that only uses metrics from the `kube-state-metrics` exporter.

{{% /details %}}

{{% /onlyWhenNot %}}

### Task {{% param sectionnumber %}}.2: Memory usage of Prometheus

**Task description**:

* Display the memory usage of both Prometheus pods
* Use a filter to just display metrics from your `prometheus` containers

{{% alert title="Note" color="primary" %}}
Search for a metric with `memory_working_set` in its name
{{% /alert %}}

{{% details title="Hints" mode-switcher="normalexpertmode" %}}
{{% onlyWhenNot baloise %}}

```promql
container_memory_working_set_bytes{pod=~"prometheus-k8s-.*", container="prometheus"}
```

Your query returns two time series per Prometheus replica with the same value but different labels. One has additional information and allows you to filter the Docker ID.

```promql
# Docker container ID as an additional information
id="/docker/34750eee3d37c8cd3594fc46bb20ed166374ece7737c590b81ed1847aaa21d50/kubepods/burstable/pode2ab8dc5-ad8e-4b5c-9b0f-49c3bb5a8a34/dce2373557fb05e0bc9819b9f786f6e3bcc882280ee2eb9267edb7040886a55b"
# Kubernetes pod information
id="/kubepods/burstable/pode2ab8dc5-ad8e-4b5c-9b0f-49c3bb5a8a34/dce2373557fb05e0bc9819b9f786f6e3bcc882280ee2eb9267edb7040886a55b"
```

{{% /onlyWhenNot %}}
{{% onlyWhen baloise %}}

```promql
container_memory_working_set_bytes{namespace="<team>-monitoring", pod=~"prometheus-prometheus-.*", container="prometheus"}
```
{{% /onlyWhen %}}

{{% /details %}}

### Task {{% param sectionnumber %}}.3: Kubernetes pod count

**Task description**:

* Display how many pods are currently running on your Kubernetes platform

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

There are different ways to archive this. You can for example query all running containers and group them by `pod` and `namespace`.

```promql
count(sum(kube_pod_container_status_running == 1) by (pod,namespace,cluster))
```

You may also sum() all running pods on your Kubernetes nodes

```promql
sum(kubelet_running_pods)
```

{{% onlyWhen baloise %}}

{{% alert title="Note" color="primary" %}}
The result counts only the pods on the clusters that you actually use. Therefore, this number may be different for each monitoring stack.
There may be a minor discrepancy between the two options, as the platform is constantly changing and different jobs query the targets at different times.
{{% /alert %}}

{{% /onlyWhen %}}

{{% /details %}}

{{% onlyWhenNot baloise %}}

### Task {{% param sectionnumber %}}.4: Add a custom Prometheus rule

The Prometheus operator allows you to extend the existing Prometheus rules with your own rules using the [PrometheusRule custom resource](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#prometheusrule). You can take a look at existing PrometheusRules or at this [example](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/alerting.md#fire-alerts) about the format of PrometheusRules.

**Task description**:

* Add a custom Prometheus rule to the monitoring stack, which checks if you have reached the defined retention size

{{% alert title="Note" color="primary" %}}
You can use the `prometheus_tsdb_size_retentions_total` metric
{{% /alert %}}

* Set the labels `prometheus: k8s` and `role: alert-rules` on your PrometheusRule to match the resource with your Prometheus configuration

See Prometheus custom resource as reference

```bash
{{% param cliToolName %}} -n monitoring edit prometheuses k8s
```

```yaml
...
spec:
  ...
  ruleSelector:
    matchLabels:
      prometheus: k8s
      role: alert-rules
  ...
```

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

There are different ways to archive this. The first approach is to just check the number of times that blocks were deleted because the maximum number of bytes was exceeded.


```bash
curl -o ~/work/prometheus-custom-rule.yml \
https://raw.githubusercontent.com/puzzle/prometheus-training/main/content/en/docs/07/labs/prometheus-custom-rule.yml
{{% param cliToolName %}} -n monitoring create -f ~/work/prometheus-custom-rule.yml
```

`prometheus-custom-rule.yml` content:
{{< readfile file="/content/en/docs/07/labs/prometheus-custom-rule.yml" code="true" lang="yaml" >}}

Another approach is to alert, when the on-disk time series database size is greater than `prometheus_tsdb_retention_limit_bytes`. For example:

```promql
prometheus_tsdb_storage_blocks_bytes >= prometheus_tsdb_retention_limit_bytes
```

In the [Prometheus web UI](http://{{% param replacePlaceholder.k8sPrometheus %}}) there is an Alerts menu item which shows you information about the configured alert.

{{% /details %}}
{{% /onlyWhenNot %}}
