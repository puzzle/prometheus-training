---
title: "7.2 Tasks: kube-prometheus setup"
weight: 7
sectionnumber: 7.2
---
The [kube-prometheus](https://github.com/prometheus-operator/kube-prometheus) stack already provides an extensive Prometheus setup and contains a set of default alerts and dashboards from [Prometheus Monitoring Mixin for Kubernetes](https://github.com/kubernetes-monitoring/kubernetes-mixin). The following targets will be available.

**kube-state-metrics:** Exposes metadata information about Kubernetes resources. Used, for example, to check if resources have the expected state (deployment rollouts, pods CrashLooping) or if jobs fail.

```promql
# Example metrics
kube_deployment_created
kube_deployment_spec_replicas
kube_daemonset_status_number_misscheduled
...
```

**kubelet/cAdvisor:** [Advisor](https://github.com/google/cadvisor) exposes usage and performance metrics about running container. Commonly used to observe memory usage or [CPU throttling](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/).

```promql
# Example metrics
container_cpu_cfs_throttled_periods_total
container_memory_working_set_bytes
container_fs_inodes_free
...
```

**kubelet:** Exposes general kubelet related metrics. Used to observe if the kubelet and the container engine is healthy.

```promql
# Example metrics
kubelet_docker_operations_duration_seconds_bucket
kubelet_runtime_operations_total
...
```

**apiserver:** Metrics from the Kubernetes API server. Commonly used to catch errors on resources or problems with latency.

```promql
# Example metrics
apiserver_request_duration_seconds_bucket
apiserver_request_total{code="200",...}
...
```

**kubelet/probes:** Expose metrics about [Kubernetes liveness, readiness and startup probes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/) Normally you would not alert on Kubernetes probe metrics, but on container restarts exposed by `kube-state-metrics`.

```promql
# Example metrics
prober_probe_total{probe_type="Liveness", result="successful",...}
prober_probe_total{probe_type="Startup", result="successful",...}
...
```

**blackbox-exporter:** Exposes default metrics from blackbox-exporter. Can be customized using the [Probe](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#probe) custom resource.

```promql
# Example metrics
probe_http_status_code
probe_http_duration_seconds
...
```

**node-exporter:** Exposes the hardware and OS metrics from the nodes running Kubernetes.

```promql
# Example metrics
node_filesystem_avail_bytes
node_disk_io_now
...
```

**alertmanager-main/grafana/prometheus-k8s/prometheus-operator/prometheus-adapter:** Exposes all monitoring stack component metrics.

```promql
# Example metrics
alertmanager_alerts_received_total
alertmanager_config_last_reload_successful
...
grafana_alerting_active_alerts
grafana_datasource_request_total
...
prometheus_config_last_reload_successful
prometheus_rule_evaluation_duration_seconds
...
prometheus_operator_reconcile_operations_total
prometheus_operator_managed_resources
...
```

### Task {{% param sectionnumber %}}.1: Container restart alerting rule

Navigate to the [Prometheus user interface](http://LOCALHOST:19090/rules) and take a look at the provided default Prometheus rules.

{{% alert title="Note" color="primary" %}}
Search for an alert with `CrashLooping` in its name
{{% /alert %}}

**Task description**:

* Investigate if there is a default Alerting rule configured to monitor container restarts
* Which exporter exposes the required metrics?

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

The Alerting rule is called `KubePodCrashLooping` and the PromQL defined for the rule looks as follows:

```promql
rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[10m]) * 60 * 5 > 0
```
If you take a look at the query, you will see that there is a filter that only uses metrics from the `kube-state-metrics` exporter.

{{% /details %}}

### Task {{% param sectionnumber %}}.2: Memory usage of Prometheus

{{% alert title="Note" color="primary" %}}
Search for an metric with `memory_working_set` in its name
{{% /alert %}}

**Task description**:

* Display the memory usage of both Prometheus pods
* Use a filter to just display metrics from the `prometheus` containers

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

```promql
container_memory_working_set_bytes{pod=~"prometheus-k8s-.*", container="prometheus"}
```

Your query returns two time series per Prometheus replica with the same value but different labels. One has additional information and allows you to filter the Docker ID.

```promql
# Docker container ID as an additional information
id="/docker/34750eee3d37c8cd3594fc46bb20ed166374ece7737c590b81ed1847aaa21d50/kubepods/burstable/pode2ab8dc5-ad8e-4b5c-9b0f-49c3bb5a8a34/dce2373557fb05e0bc9819b9f786f6e3bcc882280ee2eb9267edb7040886a55b"
# Kubernetes pod information
id="/kubepods/burstable/pode2ab8dc5-ad8e-4b5c-9b0f-49c3bb5a8a34/dce2373557fb05e0bc9819b9f786f6e3bcc882280ee2eb9267edb7040886a55b"
```

{{% /details %}}

### Task {{% param sectionnumber %}}.3: Kubernetes pod count

**Task description**:

* Display how many pods are currently running on your Kubernetes platform

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

There are different ways to archive this. You can for example query all running containers and group them by `pod` and `namespace`.

```promql
count(sum(kube_pod_container_status_running == 1) by (pod,namespace))
```

You may also sum() all running pods on your Kubernetes nodes

```promql
sum(kubelet_running_pods)
```

{{% /details %}}

### Task {{% param sectionnumber %}}.4: Add a custom Prometheus rule

The Prometheus operator allows you to extend the existing Prometheus rules with your own rules using the [PrometheusRule custom resource](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#prometheusrule). You can take a look at existing PrometheusRules or at this [example](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/alerting.md#fire-alerts) about the format of PrometheusRules.

**Task description**:

* Add a custom Prometheus rule to the monitoring stack, which checks if you have reached the defined retention size

{{% alert title="Note" color="primary" %}}
You can use the `prometheus_tsdb_size_retentions_total` metric
{{% /alert %}}

* Set the labels `prometheus: k8s` and `role: alert-rules` on your PrometheusRule to match the resource with your Prometheus configuration

See Prometheus custom resource as reference

```bash
kubectl -n monitoring edit prometheuses k8s
```

```yaml
...
spec:
  ruleSelector:
    matchLabels:
      prometheus: k8s
      role: alert-rules
...
```

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

There are different ways to archive this. The first approach is to just check the number of times that blocks were deleted because the maximum number of bytes was exceeded.

{{< highlight yaml >}}{{< readfile file="content/en/docs/07/labs/prometheus-custom-rule.yml" >}}{{< /highlight >}}

```bash
curl -o ~/work/prometheus-custom-rule.yml \
https://raw.githubusercontent.com/puzzle/prometheus-training/main/content/en/docs/07/labs/prometheus-custom-rule.yml
kubectl -n monitoring create -f ~/work/prometheus-custom-rule.yml
```

Another approach is to alert, when the on-disk time series database size is greater than `prometheus_tsdb_retention_limit_bytes`. For example:

```promql
prometheus_tsdb_storage_blocks_bytes >= prometheus_tsdb_retention_limit_bytes
```

{{% /details %}}
