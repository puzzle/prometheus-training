---
title: "7.1 Tasks: Prometheus Operator basics"
weight: 7
sectionnumber: 7.1
onlyWhenNot: baloise
---

### Task {{% param sectionnumber %}}.1: Display pod metrics in Kubernetes Grafana

The Prometheus operator stack provides a few generic dashboards for your Kubernetes cluster deployment. These dashboards provide you with information about the resource usage of Kubernetes infrastructure components or your deployed apps. They also show you latency and availability of Kubernetes core components.

**Task description**:

* Navigate to your [Kubernetes Grafana](http://{{% param replacePlaceholder.k8sGrafana %}})
* Find a dashboard that displays compute resources per namespace and pod
* Take a look at the metrics from the `monitoring` namespace

{{% alert title="Note" color="primary" %}}
The initial password is `admin`. You need to change it after the first login.
{{% /alert %}}

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

* Use the search function (magnifying glass) on the left side and hit `Search`
* The dashboard name is `Kubernetes / Compute Resources / Namespace (Pods)` and can be found in the `Default` directory
* Select `monitoring` in the namespace drop-down

You get usage metrics for CPU and memory as well as network statistics per pod in the namespace `Monitoring`.

{{% /details %}}

### Task {{% param sectionnumber %}}.2: Configure Alertmanager and Prometheus storage

By default, the Prometheus operator stack does not persist the data of the deployed monitoring stack. Therefore, any pod restart would result in a reset of all data. Let's configure persistence for Prometheus and Alertmanager.

**Task description**:

* See this [example](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/storage.md#manual-storage-provisioning) of how to configure storage for Prometheus
* Set the Prometheus volume size to `20Gi`
* Set the Alertmanager volume size to `1Gi`

{{% alert title="Note" color="primary" %}}
To get the custom resources name of your Alertmanager or Prometheus run:

```bash
kubectl -n monitoring get prometheuses
kubectl -n monitoring get alertmanagers
```

The default text editor in Kubernetes is `Vim`. If you are not familiar with `Vim`, you can switch to `Nano` or `Emacs` by setting the `KUBE_EDITOR` environment variable. Example to use `nano`:

```bash
echo 'export KUBE_EDITOR="nano"' >> ~/.bashrc
source ~/.bashrc
```

Alternatively, you can export your resources, edit them in Theia, and apply them to the Kubernetes cluster. For example:

```bash
kubectl get deployment <name> -o yaml > ~/work/deployment.yaml
kubectl apply -f ~/work/deployment.yaml
```

Custom resources can be changed by using `kubectl edit`.

```bash
kubectl -n monitoring edit <type> <name>
```

If you want to edit the Prometheus custom resource you would use

```bash
kubectl -n monitoring edit prometheuses k8s
```

{{% /alert %}}

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

Define the storage size for your Prometheis

```bash
kubectl -n monitoring edit prometheuses k8s
```

```yaml
...
spec:
  ...
  storage:
    volumeClaimTemplate:
      spec:
        resources:
          requests:
            storage: 20Gi
  ...
```

Define the storage size for your Alertmangers

```bash
kubectl -n monitoring edit alertmanagers main
```

```yaml
...
spec:
  ...
  storage:
    volumeClaimTemplate:
      spec:
        resources:
          requests:
            storage: 1Gi
  ...
```

Make sure Kubernetes provisioned the Persistent Volumes

```bash
kubectl -n monitoring get pvc
```

Check if the volume is available inside the pod with running `df -h /prometheus` inside the first Prometheus pod.

```bash
kubectl -n monitoring exec prometheus-k8s-0 -c prometheus -- df -h /prometheus
```

```bash
Filesystem                Size      Used Available Use% Mounted on
/dev/sda1                20.0G      8.4G      1.6G  84% /prometheus
```

{{% alert title="Note" color="primary" %}}
We use Minkube, which for demonstration purposes uses the `/dev/sda1` disk of your virtual machine as the storage backend for all Persistent Volumes. Therefore, you will always see the size of `/dev/sda1` when checking from inside a container. If you use an appropriate storage backend, the size inside the container will correspond to the size of your Persistent Volume.
{{% /alert %}}


{{% /details %}}

### Task {{% param sectionnumber %}}.3: Configure Prometheus Retention

By default, the Prometheus operator stack will set the retention of your metrics to `24h`. As we have now 20Gi of disk space available, we can increase the retention.
Read about [retention operational-aspects](https://prometheus.io/docs/prometheus/latest/storage/#operational-aspects) for options to manage retention.

**Task description**:

* Set Prometheus retention time to two days and retention size to 9Gi

{{% alert title="Note" color="primary" %}}
Check [documentation](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#prometheusspec) for available options
{{% /alert %}}

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

Set the following options

```bash
kubectl -n monitoring edit prometheus k8s
```

```yaml
...
spec:
  ...
  retention: 2d
  retentionSize: 9GB
  ...
```

Verify that the pods are redeployed with `kubectl -n monitoring get pods` and that the retention parameter is set in the newly created pods.

```bash
kubectl -n monitoring describe pods prometheus-k8s-0
```

The output should contain the following lines:

```yaml
...
Containers:
  ...
  prometheus:
    ...
    Args:
      ...
      --storage.tsdb.retention.size=9GB
      --storage.tsdb.retention.time=2d
      ...
```

{{% /details %}}

### Task {{% param sectionnumber %}}.4: Configure additional Alertmanager receiver

We can manage the Kubernetes Alertmanager via several approaches. In this task, we will learn how to add an additional receiver using [alertmanagerConfig custom resource](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/alerting.md#alertmanagerconfig-resource). First we need do define the alertmanagerConfigSelector label in the `Alertmanager`. This must match the labels defined in our alertmanagerConfig.

```bash
kubectl -n monitoring edit alertmanagers main
```

```yaml
...
spec:
  ...
  alertmanagerConfigSelector:
    matchLabels:
      alertmanagerConfig: training
  ...
```

**Task description**:

* Configure Alertmanger to send all alerts from the monitoring namespace to MailCatcher
* Create a `AlertmanagerConfig` custom resource. See [example](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/alerting.md#alertmanagerconfig-resource) as reference
* Name the resource `mailcatcher`
* Define the following route and receiver

```yaml
...
  route:
    receiver: 'mailcatcher'
  receivers:
    - name: 'mailcatcher'
      emailConfigs:
        - to: 'alert@localhost'
          from: 'prometheus-operator@localhost'
          smarthost: '192.168.49.1:1025'
          requireTLS: false
```

{{% alert title="Note" color="primary" %}}
When you create an `AlertmanagerConfig`, it will only match alerts that have the namespace label set to the scope in which the `AlertmanagerConfig` is defined. In our case:

```yaml
...
  route:
    ...
    routes:
    - receiver: monitoring-mailcatcher-mailcatcher
      match:
        namespace: monitoring
    ...
```

{{% /alert %}}

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

Add the AlertmanagerConfig

```bash
curl -o ~/work/mailcatcher.yml \
https://raw.githubusercontent.com/puzzle/prometheus-training/main/content/en/docs/07/labs/mailcatcher.yml

kubectl -n monitoring create -f ~/work/mailcatcher.yml
```

`mailcatcher.yml` content:
{{< readfile file="/content/en/docs/07/labs/mailcatcher.yml" code="true" lang="yaml" >}}

**Optional**: You can add an alert to check your configuration using the amtool and check if the [MailCatcher](http://{{% param replacePlaceholder.mailcatcher %}}) received the mail. It can take up to 5 minutes as the alarms are grouped together based on the [group_interval](https://prometheus.io/docs/alerting/latest/configuration/#route). E.g.

```bash
kubectl -n monitoring exec alertmanager-main-0  -c alertmanager -- \
amtool alert add --alertmanager.url=http://localhost:9093 alertname=test namespace=monitoring severity=critical
```

{{% /details %}}

### Task {{% param sectionnumber %}}.5: Check if Alertmanager is running clustered (optional)

The Prometheus operator stack deployed three Alertmanagers. Normally there would be further configuration needed to make sure that these instances running clustered. But as we are running Alertmanager managed by Prometheus operator this should be done automatically.

**Task description**: Investigate if Alertmanger is clustered and which paramaters have been set by the operator

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

The Alertmanager custom resource has 3 replicas configured

```bash
kubectl -n monitoring get alertmanager main -o yaml
```

```yaml
...
spec:
  ...
  replicas: 3
  ...
```

The operator makes sure that the Alertmanagers know about each other and generates the necessary [configuration](https://github.com/prometheus/alertmanager#high-availability) to form a cluster. Let's review the pod definition:

```bash
kubectl -n monitoring get pods alertmanager-main-0 -o yaml
```

```yaml
...
spec:
  ...
  containers:
  - args:
    ...
    - --cluster.listen-address=[$(POD_IP)]:9094
    - --cluster.peer=alertmanager-main-0.alertmanager-operated:9094
    - --cluster.peer=alertmanager-main-1.alertmanager-operated:9094
    - --cluster.peer=alertmanager-main-2.alertmanager-operated:9094
  ...
```

{{% /details %}}
