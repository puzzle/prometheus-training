---
title: "3.3 Tasks: Alertrules and alerts"
weight: 2
sectionnumber: 3.3
onlyWhenNot: baloise
---

{{% alert title="Note" color="primary" %}}

For doing the alerting lab it's useful to have a "real" application so that alerts can be provoked. The training app installed in the previous lab provides a sample app; you can start it as follows:

```bash
cd ~/work/prometheus-training-app_0.0.5_Linux_x86_64
./prometheus-training-app sampleapp &
```

The example app exposes metrics at `http://localhost:8080/metrics`

Also, the target must be registered in the Prometheus config (`/etc/prometheus/prometheus.yml`) (don't forget to reload or restart Prometheus):

```yaml
  ...
  - job_name: "sample-app"
    static_configs:
      - targets: ["localhost:8080"]
  ...
```

{{% /alert %}}

### Task {{% param sectionnumber %}}.1: Configure a target down alert

Refer to the [official documentation](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/) to see which fields you can specify for an alerting rule.

**Task description:**

* Define an alerting rule which sends an alert when a target is down. Remember the `up` metric?
* New alarms should be in `pending` state for 2 minutes before they transition to firing
* Add a label `team` with the value `team-a`
* Add an annotation `summary` with information about which instance and job is down

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

Create a new file for Prometheus `/etc/prometheus/alertrule.yml` and add the following snippet:

```yaml
groups:
- name: basic
  rules:
  - alert: Up
    expr: up == 0
    for: 2m
    labels:
      team: team-a
    annotations:
      summary: Instance {{ $labels.instance }} of job {{ $labels.job }} is down
```

The value in field `for` is the wait time until the active alert gets in state `FIRING`. Before that, the alert is `PENDING` and not yet sent to Alertmanager.

The alert is instrumented with the labels from the metric (e.g. `job`and `instance`). Additional labels can be defined in the rule. Labels can be used in Alertmanager for the routing.

With annotations, additional human-readable information can be attached to the alert.

In `/etc/prometheus/prometheus.yml`, add the rule file at `rule_files` section and restart or reload Prometheus.

```yaml
...
rule_files:
  - "recording_rules.yml"
  - "alertrule.yml"
...
```

* In the Prometheus web UI there is an **Alerts** [menu item](http://{{% param replacePlaceholder.prometheus %}}/alerts) which shows you information about the alerts.

{{% /details %}}

### Task {{% param sectionnumber %}}.2: Verify the target down alert

In this task you're going to explore what happens, when a target (our sample application) that exposes metrics fails and stops working.

* Simulate a failure by killing or stopping the sample application.
* Verify that the sample app no longer exposes metrics
* What do you observe in Prometheus UI or in Alertmanager UI?

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

You can stop the application by simply killing it:

```bash
pkill -f "prometheus-training-app sampleapp"
```

And verify that the sample application doesn't expose any metrics anymore.

```bash
curl http://localhost:8080/metrics
```

```bash
curl: (7) Failed connect to localhost:8080; Connection refused
```

* The Prometheus web UI **Alerts** [menu item](http://{{% param replacePlaceholder.prometheus %}}/alerts) shows you information about inactive and active alerts.
* As soon as an alert is in state `FIRING` the alert is sent to Alertmanager. You should see the alert in its [web UI](http://{{% param replacePlaceholder.alertmanager %}}).
* You can also check the mail in [mailcatcher](http://{{% param replacePlaceholder.mailcatcher %}})

{{% /details %}}

### Task {{% param sectionnumber %}}.3: Identify the notified receivers

Which receiver was notified when the alarm was fired?

{{% details title="Hints" mode-switcher="normalexpertmode" %}}
Receivers `receiver-a` and `receiver-b` should be notified in this case.

Explanation:

* The label `team: team-a` set on the alerting rule matches both receivers
* `continue: true` is set to true, therefore both receivers will be notified

```yaml
...
  routes:
    - receiver: 'receiver-a'
      match:
        team: 'team-a'
      continue: true
    - receiver: 'receiver-b'
      matchers:
        - team =~ "team-[a|b]"
...
```
