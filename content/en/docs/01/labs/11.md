---
title: "1.1 Tasks: Setup"
weight: 1
onlyWhenNot: baloise

sectionnumber: 1
---

In this first lab you are going to configure Prometheus to scrape the `node_exporter`.

### Task {{% param sectionnumber %}}.1: Node exporter

`node_exporter` is a Prometheus exporter for hardware and OS metrics. Or in other words, it supplies us with the more common metrics we know from classic monitoring systems.
It is therefore very useful for expanding Prometheus' monitoring capabilities into the infrastructure world.
`node_exporter` is already installed on your system and can be controlled using systemctl.

Make sure `node_exporter` is running and available at the following endpoint: `http://localhost:9100`

{{% details title="Hints" mode-switcher="normalexpertmode" %}}
So first of all, we are going make sure that `node_exporter` is running:

```bash
sudo systemctl status node_exporter.service
```
Test if the `node_exporter` works correctly:

```bash
curl http://localhost:9100
```

The above command should output this HTML page:

```html
<html>
<head><title>Node Exporter</title></head>
<body>
<h1>Node Exporter</h1>
<p><a href="/metrics">Metrics</a></p>
</body>
</html>
```

If this is the case, `node_exporter` is working correctly in your environment!
{{% /details %}}

### Task {{% param sectionnumber %}}.2: Scrape configuration

The `node_exporter` is now running and exposes hardware and OS metrics. If you'd like to check this, check `node_exporter`'s endpoint again but this time, attach the path `/metrics`:

```bash
curl http://localhost:9100/metrics
```

What you will see if you do this is a huge bunch of different values and identifiers. The values represent the actual resource consumption on your system like, e.g., consumed memory.
In order to make full use and put these metrics into perspective, we need to get them into Prometheus. We need Prometheus to _scrape_ them.

Scraping means that Prometheus regularly collects metrics from all configured endpoints, like `node_exporter`'s endpoint we just set up, parses them and saves them in its time series database. And this is what we are going to do next.

Configure Prometheus to scrape the metrics from `node_exporter`.

{{% details title="Hints" mode-switcher="normalexpertmode" %}}
There are two different ways to achieve that: Either we configure Prometheus using a static configuration or we make use of Prometheus' service discovery mechanism.


#### Option 1: Static configuration

If you decide to configure Prometheus statically, this is what it looks like:

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          # - alertmanager:9093

rule_files:

scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets: ["localhost:9090"]

  - job_name: "node_exporter"
    static_configs:
      - targets: ["localhost:9100"]
```

What's new here is the `node_exporter` part under `scrape_configs`. It adds `node_exporter`'s endpoint as a target to Prometheus.

Open your Prometheus configuration (`/etc/prometheus/prometheus.yml`) and add the new target.


#### Option 2: Service discovery

{{% alert title="Note" color="warning" %}}
Either apply option 1 or 2, not both.
{{% /alert %}}

If you decide to apply the dynamic solution, you have to adapt two files. One is `/etc/prometheus/prometheus.yml`:

```yaml
global:
  scrape_interval:     15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
  - static_configs:
    - targets:

rule_files:

scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets: ["localhost:9090"]

  - job_name: "node_exporter"
    file_sd_configs:
    - files:
      - node_exporter_targets.yml
```

The other one is `/etc/prometheus/node_exporter_targets.yml`:

```yaml
- targets:
    - 127.0.0.1:9100
```

As in the first option, Prometheus' configuration is extended by a `scrape_config`. However, it isn't a target this time but rather a reference to a file that contains the targets.


#### Reload Prometheus

Whenever you change its configuration, Prometheus needs to be reloaded to apply these changes. One way to do this is to use systemctl:

```bash
sudo systemctl reload prometheus
```

If you started Prometheus with the [-\-web.enable-lifecycle](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration) flag it's also possible to reload the Prometheus configuration using a POST request:

```bash
curl -X POST http://localhost:9090/-/reload
```

{{% alert title="Note" color="warning" %}}
Please note that `-\-web.enable-lifecycle` allows anyone with access to the Prometheus HTTP API to reload and even terminate Prometheus using the `/-/quit` endpoint.
{{% /alert %}}


#### Verify

After configuring the new target and reloading Prometheus, `node_exporter` should show up in the list of [targets](http://{{% param replacePlaceholder.prometheus %}}/targets).

{{% alert title="Hint" color="primary" %}}
Sometimes it is helpful to check if the config files are valid. You can do this with the following command. Any errors are indicated including the line number.

```bash
promtool check config /etc/prometheus/prometheus.yml
```
{{% /alert %}}

{{% /details %}}

### Task {{% param sectionnumber %}}.3: Further configuration (optional)

This task is based on the metric `net_conntrack_dialer_conn_failed_total` and consists of two parts.

* Check metrics for `net_conntrack_dialer_conn_failed_total`
* Use a metric relabel configuration to drop the metric `net_conntrack_dialer_conn_failed_total`

Verify the metric in the [Prometheus we UI](http://{{% param replacePlaceholder.prometheus %}}/graph?g0.range_input=1h&g0.expr=net_conntrack_dialer_conn_failed_total&g0.tab=1).

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

Alter the existing `prometheus` job configuration in `/etc/prometheus/prometheus.yml`
```yaml
...
  - job_name: "prometheus"

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
      - targets: ["localhost:9090"]
    metric_relabel_configs:
      - source_labels: [ __name__ ]
        regex: "net_conntrack_dialer_conn_failed_total"
        action: drop
  ...
```

Reload the configuration:

```bash
sudo systemctl reload prometheus
```

The metric still gets exposed on the exporter side, but is no longer available to query in the [Prometheus User Interface](http://{{% param replacePlaceholder.prometheus %}}/graph?g0.range_input=1h&g0.expr=net_conntrack_dialer_conn_failed_total&g0.tab=1) as it gets dropped during scrape time.

```bash
curl localhost:9090/metrics | grep net_conntrack_dialer_conn_failed_total
```

{{% /details %}}

{{% alert title="Note" color="warning" %}}
Dropping metrics should be a last resort when it is not possible to disable metrics in the first place on the exporter side. For example, you can define collectors for the [Prometheus Node Exporter](https://github.com/prometheus/node_exporter#collectors) and specify granularly which metrics should be available.

Please note that due to the so-called [staleness](https://prometheus.io/docs/prometheus/latest/querying/basics/#staleness), it may take up to 5 minutes until changes show up.
{{% /alert %}}
